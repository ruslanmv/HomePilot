# HomePilot Environment Configuration
# Copy this file to .env and edit for your setup

# =============================================================================
# FRONTEND CONFIGURATION
# =============================================================================

# Frontend API URL (where the React app sends requests)
# - For local development: http://localhost:8000
# - For docker: http://backend:8000
VITE_API_URL=http://localhost:8000

# =============================================================================
# SECURITY
# =============================================================================

# Optional API key (if set, backend requires it on all requests)
# Leave empty for no authentication (local development)
API_KEY=

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================

# Default provider: "openai_compat" (vLLM) or "ollama"
DEFAULT_PROVIDER=ollama

# --- vLLM / OpenAI-compatible endpoint ---
# Use this for high-performance LLM serving
LLM_BASE_URL=http://llm:8001/v1
LLM_MODEL=local-model

# --- Ollama (recommended for local development) ---
# Install Ollama: https://ollama.ai
# Run: ollama serve
# Pull a model: ollama pull llama3:8b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b

# =============================================================================
# IMAGE & VIDEO GENERATION (ComfyUI)
# =============================================================================

# ComfyUI server URL
# - For local development: http://localhost:8188
# - For docker: http://comfyui:8188
COMFY_BASE_URL=http://localhost:8188

# ComfyUI workflows directory (auto-detected in local dev, set explicitly if needed)
# COMFY_WORKFLOWS_DIR=/path/to/comfyui/workflows

# ComfyUI polling settings
COMFY_POLL_INTERVAL_S=1.0   # Check for completion every 1 second
COMFY_POLL_MAX_S=240        # Timeout after 4 minutes

# =============================================================================
# MEDIA PROCESSING
# =============================================================================

# Media service (FFMPEG-based) URL
MEDIA_BASE_URL=http://media:8002

# =============================================================================
# STORAGE
# =============================================================================

# SQLite database path (conversation history)
# - For local development: ./backend/data/homepilot.db
# - For docker: /app/data/homepilot.db
SQLITE_PATH=/app/data/homepilot.db

# File upload directory
UPLOAD_DIR=/app/data/uploads

# Output directory (generated images/videos)
OUTPUT_DIR=/outputs

# Max upload size (MB)
MAX_UPLOAD_MB=25

# =============================================================================
# TIMEOUTS
# =============================================================================

# General tool timeout (seconds)
TOOL_TIMEOUT_S=300

# =============================================================================
# CORS (Cross-Origin Resource Sharing)
# =============================================================================

# Allowed origins (comma-separated)
# - For local development: add your frontend URLs
# - For production: set to your domain
CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://127.0.0.1:5173

# =============================================================================
# MCP CONTEXT FORGE (Agentic Features)
# =============================================================================

# Enable MCP Gateway integration (set to false to disable)
MCP_ENABLED=true

# MCP Gateway URL (where the gateway listens)
MCP_GATEWAY_URL=http://localhost:4444

# MCP Gateway authentication (generate with: make start-mcp, then create token via Admin UI)
# MCPGATEWAY_BEARER_TOKEN=

# MCP Gateway Admin UI (required for "Open Tool & Agent Manager" link)
MCPGATEWAY_UI_ENABLED=true
MCPGATEWAY_ADMIN_API_ENABLED=true
# Disable login prompt so Admin UI opens without password (local dev)
AUTH_REQUIRED=false

# LLM provider for the LangChain agent (openai, azure, bedrock, ollama, anthropic)
# MCP_LLM_PROVIDER=ollama
# MCP_LLM_MODEL=llama3:8b

# --- Agentic AI Layer (backend → Context Forge bridge) ---
# Feature flag (true/false) — when false, /v1/agentic/* returns disabled status
AGENTIC_ENABLED=true
# Context Forge gateway URL (same as MCP_GATEWAY_URL by default)
CONTEXT_FORGE_URL=http://localhost:4444
# Admin UI URL (for "Open Tool & Agent Manager" button in Settings)
CONTEXT_FORGE_ADMIN_URL=http://localhost:4444/admin
# Pre-configured JWT Bearer token (leave empty for auto-login via /auth/login)
# CONTEXT_FORGE_TOKEN=
# Auth credentials for JWT auto-login (email derived as {user}@example.com)
# Forge requires JWT tokens — these credentials are used to acquire one automatically
CONTEXT_FORGE_AUTH_USER=admin
CONTEXT_FORGE_AUTH_PASS=changeme

# =============================================================================
# COMMUNITY GALLERY
# =============================================================================

# Primary: Cloudflare Worker proxy (edge-cached, no rate limits, clean URLs)
# Deployed with: make community-deploy-worker
COMMUNITY_GALLERY_URL=https://homepilot-persona-gallery.cloud-data.workers.dev

# Fallback: R2 direct public access (rate-limited by Cloudflare, dev use only)
# Enable public access on R2 bucket: Dashboard → R2 → bucket → Settings → Public Access
R2_PUBLIC_URL=https://pub-0274961e62694c09bdb4c8f2822ca3f1.r2.dev
#
# Priority: COMMUNITY_GALLERY_URL (Worker) takes precedence over R2_PUBLIC_URL (R2 direct).
# To fall back to R2 direct, comment out or remove COMMUNITY_GALLERY_URL.

# =============================================================================
# MCP SERVER CONFIGURATION (Ports 9101–9202)
# =============================================================================
# These variables configure the individual MCP servers started via
# docker-compose.mcp.yml or scripts/persona-launch.sh.
# All write operations are DISABLED by default (safe for local dev).

# --- Global Write-Gating (applies to all servers unless overridden) ---
WRITE_ENABLED=false
DRY_RUN=true

# --- Web Search (port 9105) ---
# Provider: "searxng" (self-hosted, free) or "tavily" (enterprise API)
WEB_SEARCH_PROVIDER=searxng
SEARXNG_BASE_URL=http://localhost:8080
# TAVILY_API_KEY=

# --- Local Notes (port 9110) ---
NOTES_WRITE_ENABLED=false
NOTES_DRY_RUN=true

# --- Local Projects (port 9111) ---
PROJECTS_WRITE_ENABLED=false
PROJECTS_DRY_RUN=true
PROJECTS_MOUNT_PATH=./data/projects
# ALLOWED_ROOTS=   # Comma-separated list of allowed project directory roots

# --- Shell Safe (port 9113) ---
SHELL_WRITE_ENABLED=false
# SHELL_EXEC_TIMEOUT=30          # Command execution timeout (seconds)
# SHELL_ALLOW_NETWORK=false      # Allow curl, wget, ssh, etc.

# --- Gmail (port 9114) ---
GMAIL_WRITE_ENABLED=false
GMAIL_CONFIRM_SEND=true

# --- Google Calendar (port 9115) ---
GCAL_WRITE_ENABLED=false

# --- Microsoft Graph (port 9116) ---
MSGRAPH_WRITE_ENABLED=false
# MS_GRAPH_CLIENT_ID=
# MS_GRAPH_CLIENT_SECRET=
# MS_GRAPH_TENANT_ID=

# --- Slack (port 9117) ---
SLACK_WRITE_ENABLED=false
# SLACK_BOT_TOKEN=

# --- GitHub (port 9118) ---
GITHUB_WRITE_ENABLED=false
# GITHUB_TOKEN=
# GITHUB_ORG=

# --- Notion (port 9119) ---
NOTION_WRITE_ENABLED=false
# NOTION_TOKEN=

# --- Persona Launcher (scripts/persona-launch.sh) ---
# HEALTH_TIMEOUT=5               # Health check timeout per server (seconds)
# STARTUP_TIMEOUT=30             # Post-startup stabilization wait (seconds)
# AUTO_REGISTER=true             # Auto-register tools in Forge after startup

# =============================================================================
# ADVANCED
# =============================================================================

# Public base URL (for generating links)
# PUBLIC_BASE_URL=https://your-domain.com

# =============================================================================
# LOCAL DEVELOPMENT QUICK START
# =============================================================================
#
# 1. Install Ollama: https://ollama.ai
# 2. Run: ollama pull llama3:8b
# 3. Start Ollama: ollama serve
# 4. Install ComfyUI: https://github.com/comfyanonymous/ComfyUI
# 5. Start ComfyUI: python main.py
# 6. Copy this file to .env
# 7. Update the following settings:
#    - DEFAULT_PROVIDER=ollama
#    - OLLAMA_BASE_URL=http://localhost:11434
#    - OLLAMA_MODEL=llama3:8b
#    - COMFY_BASE_URL=http://localhost:8188
# 8. Start backend: cd backend && uvicorn app.main:app --reload
# 9. Start frontend: cd frontend && npm run dev
# 10. Open http://localhost:3000
#
# For Grok-like "Imagine" feature:
# - Create ComfyUI workflows (see comfyui/workflows/README.md)
# - The backend will auto-detect workflows in local development
# =============================================================================
