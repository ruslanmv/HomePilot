# HomePilot Environment Configuration
# Copy this file to .env and edit for your setup

# =============================================================================
# FRONTEND CONFIGURATION
# =============================================================================

# Frontend API URL (where the React app sends requests)
# - For local development: http://localhost:8000
# - For docker: http://backend:8000
VITE_API_URL=http://localhost:8000

# =============================================================================
# SECURITY
# =============================================================================

# Optional API key (if set, backend requires it on all requests)
# Leave empty for no authentication (local development)
API_KEY=

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================

# Default provider: "openai_compat" (vLLM) or "ollama"
DEFAULT_PROVIDER=ollama

# --- vLLM / OpenAI-compatible endpoint ---
# Use this for high-performance LLM serving
LLM_BASE_URL=http://llm:8001/v1
LLM_MODEL=local-model

# --- Ollama (recommended for local development) ---
# Install Ollama: https://ollama.ai
# Run: ollama serve
# Pull a model: ollama pull llama3.1:8b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# =============================================================================
# IMAGE & VIDEO GENERATION (ComfyUI)
# =============================================================================

# ComfyUI server URL
# - For local development: http://localhost:8188
# - For docker: http://comfyui:8188
COMFY_BASE_URL=http://localhost:8188

# ComfyUI workflows directory (auto-detected in local dev, set explicitly if needed)
# COMFY_WORKFLOWS_DIR=/path/to/comfyui/workflows

# ComfyUI polling settings
COMFY_POLL_INTERVAL_S=1.0   # Check for completion every 1 second
COMFY_POLL_MAX_S=240        # Timeout after 4 minutes

# =============================================================================
# MEDIA PROCESSING
# =============================================================================

# Media service (FFMPEG-based) URL
MEDIA_BASE_URL=http://media:8002

# =============================================================================
# STORAGE
# =============================================================================

# SQLite database path (conversation history)
# - For local development: ./backend/data/homepilot.db
# - For docker: /app/data/homepilot.db
SQLITE_PATH=/app/data/homepilot.db

# File upload directory
UPLOAD_DIR=/app/data/uploads

# Output directory (generated images/videos)
OUTPUT_DIR=/outputs

# Max upload size (MB)
MAX_UPLOAD_MB=25

# =============================================================================
# TIMEOUTS
# =============================================================================

# General tool timeout (seconds)
TOOL_TIMEOUT_S=300

# =============================================================================
# CORS (Cross-Origin Resource Sharing)
# =============================================================================

# Allowed origins (comma-separated)
# - For local development: add your frontend URLs
# - For production: set to your domain
CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://127.0.0.1:5173

# =============================================================================
# ADVANCED
# =============================================================================

# Public base URL (for generating links)
# PUBLIC_BASE_URL=https://your-domain.com

# =============================================================================
# LOCAL DEVELOPMENT QUICK START
# =============================================================================
#
# 1. Install Ollama: https://ollama.ai
# 2. Run: ollama pull llama3.1:8b
# 3. Start Ollama: ollama serve
# 4. Install ComfyUI: https://github.com/comfyanonymous/ComfyUI
# 5. Start ComfyUI: python main.py
# 6. Copy this file to .env
# 7. Update the following settings:
#    - DEFAULT_PROVIDER=ollama
#    - OLLAMA_BASE_URL=http://localhost:11434
#    - OLLAMA_MODEL=llama3.1:8b
#    - COMFY_BASE_URL=http://localhost:8188
# 8. Start backend: cd backend && uvicorn app.main:app --reload
# 9. Start frontend: cd frontend && npm run dev
# 10. Open http://localhost:3000
#
# For Grok-like "Imagine" feature:
# - Create ComfyUI workflows (see comfyui/workflows/README.md)
# - The backend will auto-detect workflows in local development
# =============================================================================
